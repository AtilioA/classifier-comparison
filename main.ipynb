{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- digits: 2; - samples: 569; - features: 30\n",
      "gNB score:\n",
      " [0.96491228 0.9122807  0.94736842 0.92982456 0.94736842 0.87719298\n",
      " 0.94736842 0.94736842 0.9122807  0.91071429 0.92982456 0.96491228\n",
      " 0.98245614 0.92982456 0.96491228 0.94736842 0.92982456 0.87719298\n",
      " 0.87719298 0.92857143 0.92982456 0.96491228 0.92982456 0.92982456\n",
      " 0.92982456 0.9122807  0.92982456 0.96491228 0.96491228 0.92857143]\n",
      "gNB: Mean Accuracy: 0.93 Standard Deviation: 0.03\n",
      "gNB: Accuracy Confidence Interval (95%): (0.92, 0.94)\n",
      "\n",
      "zR score:\n",
      " [0.61403509 0.61403509 0.63157895 0.63157895 0.63157895 0.63157895\n",
      " 0.63157895 0.63157895 0.63157895 0.625      0.61403509 0.61403509\n",
      " 0.63157895 0.63157895 0.63157895 0.63157895 0.63157895 0.63157895\n",
      " 0.63157895 0.625      0.61403509 0.61403509 0.63157895 0.63157895\n",
      " 0.63157895 0.63157895 0.63157895 0.63157895 0.63157895 0.625     ]\n",
      "zR: Mean Accuracy: 0.63 Standard Deviation: 0.01\n",
      "zR: Accuracy Confidence Interval (95\\%): (0.62, 0.63)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa pacotes necessários via kernel do Jupyter\n",
    "# %pip install numpy seaborn scipy sklearn matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_digits, load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Load digits dataset from scikit-learn; split into data and target/label\n",
    "digits_data, digits_labels = load_digits(return_X_y=True)\n",
    "# Get the number of samples (lines) and features (columns); also get the number of unique labels, i.e. the number of classes\n",
    "(n_samples, n_features), n_digits = digits_data.shape, np.unique(digits_labels).size\n",
    "print(f\"- digits: {n_digits}; - samples: {n_samples}; - features: {n_features}\")\n",
    "\n",
    "# Initialize a dummy classifier (zeroR), and the Naive Bayes classifier (GaussianNB)\n",
    "zR = DummyClassifier(strategy='most_frequent')\n",
    "gNB = GaussianNB()\n",
    "\n",
    "# Initialize a scalar pipeline to scale the data before feeding it to the classifier\n",
    "scalar = StandardScaler()\n",
    "\n",
    "pipeline_gNB = Pipeline([('transformer', scalar), ('estimator', gNB)])\n",
    "pipeline_zR = Pipeline([('transformer', scalar), ('estimator', zR)])\n",
    "\n",
    "# Initialize a stratified k-fold cross-validation object with seed provided by the professor\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "# Use zeroR (DummyClassifier) and naiveBayes to predict the class of the data with stratified cross-validation (10-fold), repeated 3 times\n",
    "score_gNB = cross_val_score(pipeline_gNB, digits_data, digits_labels, scoring='accuracy', cv=RSKF)\n",
    "score_zR = cross_val_score(pipeline_zR, digits_data, digits_labels, scoring='accuracy', cv=RSKF)\n",
    "\n",
    "# Print the accuracy scores for the classifiers\n",
    "# gNB\n",
    "mean_gNB = score_gNB.mean()\n",
    "std_gNB = score_gNB.std()\n",
    "lower_gNB, upper_gNB = stats.norm.interval(0.95, loc=mean_gNB, scale=std_gNB/np.sqrt(len(score_gNB)))\n",
    "     \n",
    "print(\"gNB score:\\n\", score_gNB)\n",
    "print(\"gNB: Mean Accuracy: %0.2f Standard Deviation: %0.2f\" % (mean_gNB, std_gNB))\n",
    "print(\"gNB: Accuracy Confidence Interval (95%%): (%0.2f, %0.2f)\\n\" % (lower_gNB, upper_gNB)) \n",
    "       \n",
    "# zR                          \n",
    "mean_zR = score_zR.mean()\n",
    "std_zR = score_zR.std()\n",
    "lower_zR, upper_zR = stats.norm.interval(0.95, loc=mean_zR, \n",
    "                               scale=std_zR/np.sqrt(len(score_zR)))\n",
    "\n",
    "print(\"zR score:\\n\", score_zR)\n",
    "print(\"zR: Mean Accuracy: %0.2f Standard Deviation: %0.2f\" % (mean_zR, std_zR))\n",
    "print(\"zR: Accuracy Confidence Interval (95\\%%): (%0.2f, %0.2f)\\n\" % (lower_zR, upper_zR)) \n",
    "\n",
    "# conf_mat = confusion_matrix(digits_labels, score_gNB)\n",
    "# print(conf_mat)\n",
    "\n",
    "# plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "# for i in range(len(conf_mat)):\n",
    "#     for j in range(len(conf_mat)):\n",
    "#         plt.text(i, j, conf_mat[i][j], va=\"center\", ha=\"center\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3986472868.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(np.array([np.argmin(np.linalg.norm(self.centroid;s - x, axis=1)) for x in x_test]))\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ETAPA 2:\n",
    "# A segunda etapa consiste no treino, validação e teste dos classificadores que precisam de ajuste de hiperparâmetros, isto é, os classificadores KMC, KNN e AD\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# The KMC classifier uses a clustering algorithm to define K groups of examples of each class in the training base.\n",
    "class KMeansCentroidsClassifier(BaseEstimator):\n",
    "    def __init__(self, k=1, nClasses=1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.nClasses = nClasses\n",
    "        # self.groups = self.k * self.nClasses\n",
    "        self.centroids = []\n",
    "\n",
    "    # Assuming that a database has nClasses classes, the KMC algorithm initially forms K*nClasses groups, with K groups in each of the nClasses classes. The centroids of each of the groups are calculated and this centroid is associated with the class of the group from which it was generated. The method has the value of K as a hyperparameter.\n",
    "    def fit(self, x_train, y_train):\n",
    "        # x_train, y_train = check_X_y(x_train, y_train)\n",
    "\n",
    "        # Initialize nClasses:\n",
    "        self.nClasses = len(np.unique(y_train))\n",
    "\n",
    "        for _class in range(self.nClasses):\n",
    "            # print(x_train)\n",
    "            # Initialize KMeans:\n",
    "            km = KMeans(n_clusters=self.k, random_state=36851234)\n",
    "            # Fit KMeans:\n",
    "            km.fit(x_train[y_train == _class], y_train[y_train == _class])\n",
    "            # Append centroids to centroids list:   \n",
    "            self.centroids.append(km.cluster_centers_)\n",
    "        return self\n",
    "\n",
    "        # KMC calculates the centroids of each of the classes:\n",
    "        # KMC associates each of the centroids with the class of the group from which it was generated:\n",
    "        # self.labels = np.array([np.argmin(np.linalg.norm(self.centroids - self.centroids[i], axis=1)) for i in range(self.nClasses * self.k)])\n",
    "        # self.centroids = self.kmeans.cluster_centers_\n",
    "        # self.labels = self.kmeans.labels_\n",
    "\n",
    "    # To perform a classification, KMC checks which centroid is closest to the element to be classified and returns its class. To create the KMC method, sklearn's Kmeans method (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) must be used with default values for its hyperparameters.\n",
    "    def predict(self, x_test):\n",
    "        # Percorrer x_test\n",
    "        \n",
    "        # Para cada cluster, calcular a distância do elemento x_test ao centroide\n",
    "        # Retornar o cluster mais próximo\n",
    "        distancias = [np.linalg.norm(x_test-self.cents[centroide]) for centroide in self.centroids]\n",
    "        classificacao = distancias.index(min(distancias))\n",
    "        # for centroid in self.centroids:\n",
    "            # print(np.linalg.norm(centroid - x_test, axis=1))\n",
    "        # Obter a classificação do cluster mais próximo\n",
    "         \n",
    "        # KMC checks which centroid is closest to the element to be classified and returns its class:\n",
    "\n",
    "\n",
    "dKNN = KNeighborsClassifier(weights='distance')\n",
    "pipeline_kNN = Pipeline([('transformer', scalar), ('estimator', dKNN)])\n",
    "KMC = KMeansCentroidsClassifier()\n",
    "pipeline_KMC = Pipeline([('transformer', scalar), ('estimator', KMC)])\n",
    "AD = DecisionTreeClassifier()\n",
    "pipeline_AD = Pipeline([('transformer', scalar), ('estimator', AD)])\n",
    "\n",
    "# Neste caso, o procedimento de treinamento, validação e teste será realizado através de 3 rodadas de ciclos aninhados de validação e teste,\n",
    "# com o ciclo interno de validação contendo 4 folds e o externo de teste com 10 folds.\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "\n",
    "# A busca em grade (grid search) do ciclo interno deve considerar os seguintes valores de hiperparâmetros de cada técnica de aprendizado:\n",
    "# KMC: [k = 1, 3, 5, 7]\n",
    "# KNN: [n_neighbors = 1, 3, 5, 7]\n",
    "# AD: [max_depth = None, 3, 5, 10]\n",
    "grade_KMC = {'estimator__k': [1, 3, 5, 7]}\n",
    "grade_kNN = {'estimator__n_neighbors': [1, 3, 5, 7]}\n",
    "grade_AD = {'estimator__max_depth': [None, 3, 5, 10]}\n",
    "grid_search_KMC = GridSearchCV(estimator=pipeline_KMC, param_grid=grade_KMC, scoring='accuracy', cv=4)\n",
    "grid_search_kNN = GridSearchCV(estimator=pipeline_kNN, param_grid=grade_kNN, scoring='accuracy', cv=4)\n",
    "grid_search_AD = GridSearchCV(estimator=pipeline_AD, param_grid=grade_AD, scoring='accuracy', cv=4)\n",
    "\n",
    "# Os dados utilizados no conjunto de treino em cada rodada de teste devem ser padronizados (normalização com z-score).\n",
    "# Os valores de padronização obtidos nos dados de treino devem ser utilizados para padronizar os dados do respectivo conjunto de teste.\n",
    "# ?\n",
    "\n",
    "scores_kNN = cross_val_score(grid_search_kNN, digits_data, digits_labels, scoring='accuracy', cv=RSKF)\n",
    "scores_AD = cross_val_score(grid_search_AD, digits_data, digits_labels, scoring='accuracy', cv=RSKF)\n",
    "scores_KMC = cross_val_score(grid_search_KMC, digits_data, digits_labels, scoring='accuracy', cv=RSKF)\n",
    "\n",
    "# Os resultados de cada classificador devem ser apresentados numa tabela contendo a média das acurácias obtidas em cada fold, o desvio padrão e o intervalo de confiança a 95% de significância dos resultados,\n",
    "\n",
    "# e também através do boxplot dos resultados de cada classificador em cada fold.\n",
    "# Os gráficos bloxplot requeridos no treino e no teste devem ser gerados usando função específica do pacote seaborn\n",
    "def example1():\n",
    "    mydata = [1, 2, 3, 4, 5, 6, 12]\n",
    "    sns.boxplot(y=mydata)  # Also accepts numpy arrays\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def example2():\n",
    "    df = sns.load_dataset('iris')\n",
    "    # returns a DataFrame object. This dataset has 150 examples.\n",
    "    # print(df)\n",
    "    # Make boxplot for each group\n",
    "    sns.boxplot(data=df.loc[:, :])\n",
    "    # loc[:,:] means all lines and all columns\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# example1()\n",
    "# example2()\n",
    "\n",
    "# será necessário apresentar também a tabela pareada dos resultados (p-values) dos testes de hipótese entre os pares de métodos.\n",
    "# Na matriz triangular superior devem ser apresentados os resultados do teste t pareado (amostras dependentes).\n",
    "# Na matriz triangular inferior devem ser apresentados os resultado do teste não paramétrico de wilcoxon.\n",
    "# Os valores da célula da tabela rejeitarem a hipótese nula para um nível de significância de 95% devem ser escritos em negrito.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
