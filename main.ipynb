{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          zR       NBG\n",
      "0   0.100000  0.777778\n",
      "1   0.100000  0.805556\n",
      "2   0.100000  0.805556\n",
      "3   0.100000  0.750000\n",
      "4   0.100000  0.722222\n",
      "5   0.100000  0.766667\n",
      "6   0.100000  0.766667\n",
      "7   0.100559  0.815642\n",
      "8   0.106145  0.821229\n",
      "9   0.106145  0.826816\n",
      "10  0.100000  0.777778\n",
      "11  0.100000  0.783333\n",
      "12  0.100000  0.783333\n",
      "13  0.100000  0.777778\n",
      "14  0.100000  0.744444\n",
      "15  0.100000  0.761111\n",
      "16  0.100000  0.755556\n",
      "17  0.100559  0.770950\n",
      "18  0.106145  0.798883\n",
      "19  0.106145  0.810056\n",
      "20  0.100000  0.811111\n",
      "21  0.100000  0.738889\n",
      "22  0.100000  0.761111\n",
      "23  0.100000  0.800000\n",
      "24  0.100000  0.838889\n",
      "25  0.100000  0.777778\n",
      "26  0.100000  0.777778\n",
      "27  0.100559  0.854749\n",
      "28  0.106145  0.754190\n",
      "29  0.106145  0.793296\n",
      "zR-NBG Paired T Test:\n",
      "p-value: 4.230984524662128e-41\n",
      "zR-NBG Wilcoxon Test:\n",
      " p-value: 1.7083716990437344e-06\n",
      "\\textbf{Método} & \\textbf{Média} & \\textbf{Desvio Padrão} & \\textbf{Limite Inferior} & \\textbf{Limite Superior} \\ \\hline \n",
      "ZR & 0.10 & 0.00 & 0.10 & 0.10 \\ \\hline \n",
      "NGB & 0.78 & 0.03 & 0.77 & 0.80 \\ \\hline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa pacotes necessários via kernel do Jupyter\n",
    "# %pip install numpy seaborn scipy sklearn matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "from sklearn.datasets import load_digits, load_breast_cancer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "def create_stats_header():\n",
    "    return '\\\\textbf{Método} & \\\\textbf{Média} & \\\\textbf{Desvio Padrão} & \\\\textbf{Limite Inferior} & \\\\textbf{Limite Superior} \\\\ \\hline \\n'\n",
    "\n",
    "def create_stats_table_line(method_name, mean, std, lower, upper):\n",
    "    return '{} & {:.2f} & {:.2f} & {:.2f} & {:.2f} \\\\ \\hline \\n'.format(method_name, mean, std, lower, upper)\n",
    "\n",
    "# Load digits dataset from scikit-learn; split into data and target/label\n",
    "data, labels = load_digits(return_X_y=True)\n",
    "# Get the number of samples (lines) and features (columns); also get the number of unique labels, i.e. the number of classes\n",
    "(n_samples, n_features), n_digits = data.shape, np.unique(labels).size\n",
    "# print(f\"- digits: {n_digits}; - samples: {n_samples}; - features: {n_features}\")\n",
    "\n",
    "# Initialize a dummy classifier (zeroR), and the Naive Bayes classifier (GaussianNB)\n",
    "zR = DummyClassifier(strategy='most_frequent')\n",
    "NBG = GaussianNB()\n",
    "\n",
    "# Initialize a scalar pipeline to scale the data before feeding it to the classifier\n",
    "scalar = StandardScaler()\n",
    "\n",
    "pipeline_zR = Pipeline([('transformer', scalar), ('estimator', zR)])\n",
    "pipeline_NBG = Pipeline([('transformer', scalar), ('estimator', NBG)])\n",
    "\n",
    "# Initialize a stratified k-fold cross-validation object with seed provided by the professor\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "# Use zeroR (DummyClassifier) and naiveBayes to predict the class of the data with stratified cross-validation (10-fold), repeated 3 times\n",
    "scores_zR = cross_val_score(pipeline_zR, data, labels, scoring='accuracy', cv=RSKF)\n",
    "scores_NBG = cross_val_score(pipeline_NBG, data, labels, scoring='accuracy', cv=RSKF)\n",
    "\n",
    "# Create a dataframe where the columns are the methods and the rows are the scores\n",
    "df_scores = pd.DataFrame(data={'zR': scores_zR, 'NBG': scores_NBG})\n",
    "print(df_scores)\n",
    "\n",
    "# Print the accuracy scores for the classifiers\n",
    "# NBG\n",
    "mean_NBG = scores_NBG.mean()\n",
    "std_NBG = scores_NBG.std()\n",
    "lower_NBG, upper_NBG = stats.norm.interval(0.95, loc=mean_NBG, scale=std_NBG/np.sqrt(len(scores_NBG)))\n",
    "     \n",
    "# print(\"NBG score:\\n\", scores_NBG)\n",
    "# print(\"NBG: Mean Accuracy: %0.2f Standard Deviation: %0.2f\" % (mean_NBG, std_NBG))\n",
    "# print(\"NBG: Accuracy Confidence Interval (95%%): (%0.2f, %0.2f)\\n\" % (lower_NBG, upper_NBG)) \n",
    "       \n",
    "# zR                          \n",
    "mean_zR = scores_zR.mean()\n",
    "std_zR = scores_zR.std()\n",
    "lower_zR, upper_zR = stats.norm.interval(0.95, loc=mean_zR, \n",
    "                               scale=std_zR/np.sqrt(len(scores_zR)))\n",
    "                            \n",
    "\n",
    "# print(\"zR score:\\n\", scores_zR)\n",
    "# print(\"zR: Mean Accuracy: %0.2f Standard Deviation: %0.2f\" % (mean_zR, std_zR))\n",
    "# print(\"zR: Accuracy Confidence Interval (95\\%%): (%0.2f, %0.2f)\\n\" % (lower_zR, upper_zR)) \n",
    "\n",
    "\n",
    "# ZR\n",
    "_, pTValueZrNBG = ttest_rel(scores_zR, scores_NBG)\n",
    "print(f'zR-NBG Paired T Test:\\np-value: {pTValueZrNBG}')\n",
    "\n",
    "_, pWValueZrNBG = wilcoxon(scores_zR, scores_NBG)\n",
    "print(f'zR-NBG Wilcoxon Test:\\n p-value: {pWValueZrNBG}')\n",
    "\n",
    "table = create_stats_header()\n",
    "table += create_stats_table_line('ZR', mean_zR, std_zR, lower_zR, upper_zR)\n",
    "table += create_stats_table_line('NGB', mean_NBG, std_NBG, lower_NBG, upper_NBG)\n",
    "print(table)\n",
    "\n",
    "# conf_mat = confusion_matrix(labels, score_NBG)\n",
    "# print(conf_mat)\n",
    "\n",
    "# plt.matshow(conf_mat, cmap=plt.cm.Blues)\n",
    "# for i in range(len(conf_mat)):\n",
    "#     for j in range(len(conf_mat)):\n",
    "#         plt.text(i, j, conf_mat[i][j], va=\"center\", ha=\"center\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMC score:\n",
      " [0.88888889 0.95555556 0.97222222 0.95       0.92777778 0.95555556\n",
      " 0.96111111 0.96089385 0.94972067 0.97765363 0.95       0.95555556\n",
      " 0.96666667 0.92777778 0.98333333 0.96666667 0.93333333 0.96648045\n",
      " 0.94972067 0.96648045 0.93888889 0.98888889 0.97222222 0.95\n",
      " 0.96111111 0.94444444 0.95       0.93854749 0.97206704 0.94972067]\n",
      "KMC: Mean Accuracy: 0.95 Standard Deviation: 0.02\n",
      "KMC: Accuracy Confidence Interval (95\\%): (0.95, 0.96)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ETAPA 2:\n",
    "# A segunda etapa consiste no treino, validação e teste dos classificadores que precisam de ajuste de hiperparâmetros, isto é, os classificadores KMC, KNN e AD\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_X_y\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# The KMC classifier uses a clustering algorithm to define K groups of examples of each class in the training base.\n",
    "class KMeansCentroidsClassifier(BaseEstimator):\n",
    "    def __init__(self, k=1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.centroids = []\n",
    "        # self.groups = self.k * self.nClasses\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        x_train, y_train = check_X_y(x_train, y_train)\n",
    "\n",
    "        for _class in np.unique(y_train):\n",
    "            # Initialize KMeans:\n",
    "            km = KMeans(n_clusters=self.k)\n",
    "            # Fit KMeans:\n",
    "            km.fit(x_train[y_train == _class], y_train[y_train == _class])\n",
    "            # Append centroids to centroids list:   \n",
    "            self.centroids.append({\"clusters\": km.cluster_centers_, \"class\": _class})\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        classes = []\n",
    "        for x in x_test:\n",
    "            min_dist = np.Inf\n",
    "            \n",
    "            for centroid in self.centroids:\n",
    "                for cluster in centroid[\"clusters\"]:\n",
    "                    dist = np.linalg.norm(x - cluster)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        _class = centroid[\"class\"]\n",
    "\n",
    "            classes.append(_class)\n",
    "            \n",
    "        return classes\n",
    "                    \n",
    "\n",
    "dKNN = KNeighborsClassifier(weights='distance')\n",
    "pipeline_kNN = Pipeline([('transformer', scalar), ('estimator', dKNN)])\n",
    "KMC = KMeansCentroidsClassifier()\n",
    "pipeline_KMC = Pipeline([('transformer', scalar), ('estimator', KMC)])\n",
    "AD = DecisionTreeClassifier()\n",
    "pipeline_AD = Pipeline([('transformer', scalar), ('estimator', AD)])\n",
    "\n",
    "# Neste caso, o procedimento de treinamento, validação e teste será realizado através de 3 rodadas de ciclos aninhados de validação e teste,\n",
    "# com o ciclo interno de validação contendo 4 folds e o externo de teste com 10 folds.\n",
    "RSKF = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "\n",
    "# A busca em grade (grid search) do ciclo interno deve considerar os seguintes valores de hiperparâmetros de cada técnica de aprendizado:\n",
    "# KMC: [k = 1, 3, 5, 7]\n",
    "# KNN: [n_neighbors = 1, 3, 5, 7]\n",
    "# AD: [max_depth = None, 3, 5, 10]\n",
    "grade_KMC = {'estimator__k': [1, 3, 5, 7]}\n",
    "grade_kNN = {'estimator__n_neighbors': [1, 3, 5, 7]}\n",
    "grade_AD = {'estimator__max_depth': [None, 3, 5, 10]}\n",
    "grid_search_KMC = GridSearchCV(estimator=pipeline_KMC, param_grid=grade_KMC, scoring='accuracy', cv=4)\n",
    "grid_search_kNN = GridSearchCV(estimator=pipeline_kNN, param_grid=grade_kNN, scoring='accuracy', cv=4)\n",
    "grid_search_AD = GridSearchCV(estimator=pipeline_AD, param_grid=grade_AD, scoring='accuracy', cv=4)\n",
    "\n",
    "# Os dados utilizados no conjunto de treino em cada rodada de teste devem ser padronizados (normalização com z-score).\n",
    "# Os valores de padronização obtidos nos dados de treino devem ser utilizados para padronizar os dados do respectivo conjunto de teste.\n",
    "# ?\n",
    "\n",
    "scores_kNN = cross_val_score(grid_search_kNN, data, labels, scoring='accuracy', cv=RSKF)\n",
    "scores_AD = cross_val_score(grid_search_AD, data, labels, scoring='accuracy', cv=RSKF)\n",
    "scores_KMC = cross_val_score(grid_search_KMC, data, labels, scoring='accuracy', cv=RSKF)\n",
    "\n",
    "mean_KMC = scores_KMC.mean()\n",
    "std_KMC = scores_KMC.std()\n",
    "lower_KMC, upper_KMC = stats.norm.interval(0.95, loc=mean_KMC, scale=std_KMC/np.sqrt(len(scores_KMC)))\n",
    "\n",
    "print(\"KMC score:\\n\", scores_KMC)\n",
    "print(\"KMC: Mean Accuracy: %0.2f Standard Deviation: %0.2f\" % (mean_KMC, std_KMC))\n",
    "print(\"KMC: Accuracy Confidence Interval (95\\%%): (%0.2f, %0.2f)\\n\" % (lower_KMC, upper_KMC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AD': [{'p': 1.0796784545713246e-17, 'w': 1.7224282827430733e-06}],\n",
      " 'KMC': [],\n",
      " 'NBG': [{'p': 3.7223941692129973e-25, 'w': 1.7224282827430733e-06},\n",
      "         {'p': 2.585786419750758e-10, 'w': 2.1122012156048867e-06},\n",
      "         {'p': 3.3241854029509423e-21, 'w': 1.7289484360195754e-06}],\n",
      " 'kNN': [{'p': 2.849455595993415e-24, 'w': 1.7213435893906393e-06},\n",
      "         {'p': 4.085040755090073e-05, 'w': 0.00011836231046420427}],\n",
      " 'zR': [{'p': 4.230984524662128e-41, 'w': 1.7083716990437344e-06},\n",
      "        {'p': 1.557666013355992e-54, 'w': 1.6858673414334802e-06},\n",
      "        {'p': 1.937295275155675e-47, 'w': 1.711606992411454e-06},\n",
      "        {'p': 1.762581599344587e-49, 'w': 1.7105279932400777e-06}]}\n",
      "Statistical tests:\n",
      "[['zR', 0.0, 0.0, 0.0, 0.0], [1.71e-06, 'NBG', 0.0, 0.0, 0.0], [1.69e-06, 1.72e-06, 'kNN', 0.0, 4.085e-05], [1.71e-06, 2.11e-06, 1.72e-06, 'AD', 0.0], [1.71e-06, 1.73e-06, 0.00011836, 1.72e-06, 'KMC']]\n",
      "\\hline\n",
      "zR & \\textbf{0.0} & \\textbf{0.0} & \\textbf{0.0} & \\textbf{0.0} \\\\ \\hline\n",
      "\\textbf{1.71e-06} & NBG & \\textbf{0.0} & \\textbf{0.0} & \\textbf{0.0} \\\\ \\hline\n",
      "\\textbf{1.69e-06} & \\textbf{1.72e-06} & kNN & \\textbf{0.0} & \\textbf{4.085e-05} \\\\ \\hline\n",
      "\\textbf{1.71e-06} & \\textbf{2.11e-06} & \\textbf{1.72e-06} & AD & \\textbf{0.0} \\\\ \\hline\n",
      "\\textbf{1.71e-06} & \\textbf{1.73e-06} & \\textbf{0.00011836} & \\textbf{1.72e-06} & KMC \\\\ \\hline\n",
      "[['zR', 0.0, 0.0, 0.0, 0.0],\n",
      "[1.71e-06, 'NBG', 0.0, 0.0, 0.0],\n",
      "[1.69e-06, 1.72e-06, 'kNN', 0.0,\n",
      "4.085e-05],\n",
      "[1.71e-06, 2.11e-06, 1.72e-06, 'AD',\n",
      "0.0],\n",
      "[1.71e-06, 1.73e-06, 0.00011836,\n",
      "1.72e-06, 'KMC']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def test_two_models(scores1, scores2):\n",
    "    pTValue = ttest_rel(scores1, scores2)\n",
    "    pWValue = wilcoxon(scores1, scores2)\n",
    "    return pTValue, pWValue\n",
    "\n",
    "\n",
    "def test_models(scores):\n",
    "    # Test each model with the other, one by one:\n",
    "    pTValues = []\n",
    "    pWValues = []\n",
    "\n",
    "    testsDict = {}\n",
    "    for i, (model, score) in enumerate(scores.items()):\n",
    "        testsDict[model] = []\n",
    "        for j in range(i+1, len(scores)):\n",
    "            pTValue, pWValue = test_two_models(score, scores[list(scores.keys())[j]])\n",
    "            testsDict[model].append({'p': pTValue.pvalue, 'w': pWValue.pvalue})\n",
    "\n",
    "    pprint(testsDict)\n",
    "    return testsDict\n",
    "\n",
    "\n",
    "def get_statistical_tests_matrix(statisticalTestsDict):\n",
    "    MIN_CONFIDENCE_LEVEL = 0.05\n",
    "    statisticalTestMatrix = [[None for _ in range(len(statisticalTestsDict))] for _ in range(len(statisticalTestsDict))]\n",
    "    \n",
    "    print(\"Statistical tests:\")\n",
    "    \n",
    "    # Iterate over statisticalTestsDict in a cross fashion:\n",
    "    for i, (model, tests) in enumerate(statisticalTestsDict.items()):\n",
    "        # Print model name in the diagonal:\n",
    "        j = i\n",
    "        statisticalTestMatrix[i][j] = model\n",
    "        for test in tests:\n",
    "            j += 1\n",
    "            statisticalTestMatrix[i][j] = round(test['p'], 8)\n",
    "            statisticalTestMatrix[j][i] = round(test['w'], 8)\n",
    "                \n",
    "    return statisticalTestMatrix\n",
    "\n",
    "\n",
    "def create_matrix_table(statisticalTestMatrix):\n",
    "    print(\"\\hline\")\n",
    "    for row in statisticalTestMatrix:\n",
    "        for i, cell in enumerate(row):\n",
    "            ending = None\n",
    "            # If cell is not the last in the row:\n",
    "            if i != len(row) - 1:\n",
    "                ending = ' & '\n",
    "            else: \n",
    "                ending = ' \\\\\\\\ \\hline\\n'\n",
    "                \n",
    "            if type(cell) is not str:\n",
    "                if cell > 0.05:\n",
    "                    print(cell, end=ending)\n",
    "                else:\n",
    "                    print(f'\\\\textbf{{{cell}}}', end=ending)\n",
    "            else: \n",
    "                print(cell, end=ending)\n",
    "\n",
    "\n",
    "scoresDict = {\n",
    "    'zR': scores_zR,\n",
    "    'NBG': scores_NBG,\n",
    "    'kNN': scores_kNN,\n",
    "    'AD': scores_AD,\n",
    "    'KMC': scores_KMC,\n",
    "}\n",
    "\n",
    "statisticalTestsDict = test_models(scoresDict)\n",
    "\n",
    "# Print the statistical tests results. The diagonal of the matrix contains the names of the models. The upper diagonal contains the p-values of the statistical tests. The lower diagonal contains the Wilcoxon p-values.\n",
    "statisticalTestMatrix = get_statistical_tests_matrix(statisticalTestsDict)\n",
    "print(statisticalTestMatrix)\n",
    "create_matrix_table(statisticalTestMatrix)\n",
    "pprint(statisticalTestMatrix, width=40, indent=0, compact=True)\n",
    "\n",
    "# Add KNN, AD and KMC scores to df_scores:\n",
    "df_scores['KMC'] = scores_KMC\n",
    "df_scores['KNN'] = scores_kNN\n",
    "df_scores['AD'] = scores_AD\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
